|대주제|중주제|소주제|논문 이름|주소|
|:----------------:|:----------------:|:----------------:|:----------------:|:----------------:|
|NLP|Embedding|Word2Vec|Efficient Estimation of Word Representations in Vector Space|https://arxiv.org/pdf/1301.3781.pdf%C3%AC%E2%80%94%20%C3%AC%E2%80%9E%C5%93|
|NLP|Embedding|FastText|Enriching Word Vectors with Subword Information|https://arxiv.org/abs/1607.04606v2|
|NLP|Embedding|Glove|GloVe: Global Vectors for Word Representation|https://aclanthology.org/D14-1162/|
|NLP|Embedding|ELMO|Deep contextualized word representations|https://arxiv.org/abs/1802.05365v2|
|NLP|Model/RNN|RNN|||
|NLP|Model/RNN|LSTM|||
|NLP|Model/RNN|GRU|Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation|https://arxiv.org/abs/1406.1078v3|
|NLP|Model/RNN|Attention|||
|NLP|Model/Transformer|Transformer|Attention Is All You Need|https://arxiv.org/abs/1706.03762v5|https://github.com/tunz/transformer-pytorch/blob/e7266679f0b32fd99135ea617213f986ceede056/model/transformer.py#L201|
|NLP|Model/Transformer|GPT-1|Improving Language Understanding by Generative Pre-Training|https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf||
|NLP|Model/Transformer|GPT-2|Language Models are Unsupervised Multitask Learners|https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf||
|NLP|Model/Transformer|GPT-3|Language Models are Few-Shot Learners|https://arxiv.org/abs/2005.14165v4||
|NLP|Model/Transformer|BERT|BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|https://arxiv.org/abs/1810.04805v2|https://github.com/google-research/bert|
|NLP|Model/Transformer|RoBERTa|RoBERTa: A Robustly Optimized BERT Pretraining Approach|https://arxiv.org/abs/1907.11692v1|
|NLP|Model/Transformer|XLNet|XLNet: Generalized Autoregressive Pretraining for Language Understanding|https://arxiv.org/abs/1906.08237v2|
|NLP|Model/Transformer|ALBERT|ALBERT: A Lite BERT for Self-supervised Learning of Language Representations|https://arxiv.org/abs/1909.11942v6|
|NLP|Model/Transformer|DistillBERT|DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter|https://arxiv.org/abs/1910.01108v4|
